"""
4ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
TF-IDFì™€ TextRank ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import os
import json
from pathlib import Path
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx


class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œê¸° - TF-IDF & TextRank"""
    
    def __init__(self, morpheme_folder="output/morpheme", output_folder="output/keywords"):
        self.morpheme_folder = morpheme_folder
        self.output_folder = output_folder
        os.makedirs(output_folder, exist_ok=True)
        
        print("í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    def load_morpheme_file(self, morpheme_path):
        """í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ"""
        try:
            with open(morpheme_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def extract_tfidf_keywords(self, documents, top_n=10):
        """
        TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê° ë¬¸ì„œëŠ” ë‹¨ì–´ë“¤ì˜ ë¬¸ìì—´)
            top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
        
        Returns:
            ê° ë¬¸ì„œë³„ í‚¤ì›Œë“œì™€ ì ìˆ˜
        """
        
        if len(documents) < 2:
            print("  âš ï¸  TF-IDFëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ë¬¸ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return None
        
        # TF-IDF ë²¡í„°ë¼ì´ì €
        vectorizer = TfidfVectorizer(
            max_features=1000,  # ìµœëŒ€ 1000ê°œ ë‹¨ì–´
            min_df=1,           # ìµœì†Œ 1ê°œ ë¬¸ì„œì— ë“±ì¥
            max_df=0.8          # 80% ì´ìƒ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ ì œì™¸
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(documents)
            feature_names = vectorizer.get_feature_names_out()
            
            results = []
            
            for i in range(len(documents)):
                scores = tfidf_matrix[i].toarray()[0]
                
                # ì ìˆ˜ì™€ ë‹¨ì–´ ë§¤ì¹­
                word_scores = [
                    (feature_names[j], float(scores[j])) 
                    for j in range(len(scores)) if scores[j] > 0
                ]
                
                # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
                word_scores.sort(key=lambda x: x[1], reverse=True)
                
                results.append(word_scores[:top_n])
            
            return results
        
        except Exception as e:
            print(f"  âŒ TF-IDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def extract_textrank_keywords(self, words, top_n=10, window=5):
        """
        TextRank ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            words: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
            top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
            window: ë™ì‹œ ë“±ì¥ ìœˆë„ìš° í¬ê¸°
        
        Returns:
            í‚¤ì›Œë“œì™€ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        """
        
        if len(words) < 5:
            print("  âš ï¸  TextRankëŠ” ìµœì†Œ 5ê°œ ì´ìƒì˜ ë‹¨ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return None
        
        # ê·¸ë˜í”„ ìƒì„±
        graph = nx.Graph()
        
        # ë…¸ë“œ ì¶”ê°€ (ë‹¨ì–´)
        unique_words = list(set(words))
        graph.add_nodes_from(unique_words)
        
        # ì—£ì§€ ì¶”ê°€ (ë™ì‹œ ë“±ì¥)
        for i in range(len(words)):
            for j in range(i + 1, min(i + window, len(words))):
                if words[i] != words[j]:
                    if graph.has_edge(words[i], words[j]):
                        # ê¸°ì¡´ ì—£ì§€ ê°€ì¤‘ì¹˜ ì¦ê°€
                        graph[words[i]][words[j]]['weight'] += 1
                    else:
                        # ìƒˆ ì—£ì§€ ì¶”ê°€
                        graph.add_edge(words[i], words[j], weight=1)
        
        # PageRank ì•Œê³ ë¦¬ì¦˜ ì ìš©
        try:
            scores = nx.pagerank(graph, weight='weight')
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            
            return sorted_words[:top_n]
        
        except Exception as e:
            print(f"  âŒ TextRank ì‹¤íŒ¨: {e}")
            return None
    
    def extract_frequency_keywords(self, words, top_n=10):
        """
        ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ë³¸)
        
        Args:
            words: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
            top_n: ì¶”ì¶œí•  í‚¤ì›Œë“œ ìˆ˜
        
        Returns:
            í‚¤ì›Œë“œì™€ ë¹ˆë„ ë¦¬ìŠ¤íŠ¸
        """
        counter = Counter(words)
        return counter.most_common(top_n)
    
    def analyze_single_file(self, morpheme_filename):
        """ë‹¨ì¼ íŒŒì¼ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        morpheme_path = os.path.join(self.morpheme_folder, morpheme_filename)
        
        if not os.path.exists(morpheme_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {morpheme_path}")
            return None
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘: {morpheme_filename}")
        print('='*60)
        
        # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        morpheme_data = self.load_morpheme_file(morpheme_path)
        if not morpheme_data:
            return None
        
        # ëª…ì‚¬ë§Œ ì‚¬ìš© (í‚¤ì›Œë“œëŠ” ëŒ€ë¶€ë¶„ ëª…ì‚¬)
        nouns = morpheme_data.get('all_nouns', [])
        
        print(f"   ëª…ì‚¬ ê°œìˆ˜: {len(nouns)}ê°œ")
        
        if len(nouns) < 5:
            print(f"   âš ï¸  ëª…ì‚¬ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
            return None
        
        # 1. ë¹ˆë„ ê¸°ë°˜
        print(f"\n   ğŸ“Š ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ:")
        freq_keywords = self.extract_frequency_keywords(nouns, top_n=10)
        for word, count in freq_keywords[:5]:
            print(f"      {word}: {count}íšŒ")
        
        # 2. TextRank
        print(f"\n   ğŸ•¸ï¸  TextRank ê¸°ë°˜ í‚¤ì›Œë“œ:")
        textrank_keywords = self.extract_textrank_keywords(nouns, top_n=10)
        if textrank_keywords:
            for word, score in textrank_keywords[:5]:
                print(f"      {word}: {score:.4f}")
        
        # ê²°ê³¼ ì €ì¥
        output_data = {
            'filename': morpheme_data.get('filename', ''),
            'total_nouns': len(nouns),
            'frequency_keywords': freq_keywords,
            'textrank_keywords': textrank_keywords if textrank_keywords else []
        }
        
        output_filename = Path(morpheme_filename).stem.replace('_morpheme', '_keywords.json')
        output_path = os.path.join(self.output_folder, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n   ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        
        return output_data
    
    def analyze_all_files(self):
        """ëª¨ë“  íŒŒì¼ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        morpheme_files = sorted([
            f for f in os.listdir(self.morpheme_folder) 
            if f.endswith('_morpheme.json')
        ])
        
        if not morpheme_files:
            print(f"âŒ í˜•íƒœì†Œ ë¶„ì„ íŒŒì¼ ì—†ìŒ: {self.morpheme_folder}")
            return []
        
        print(f"\nğŸ“š ì´ {len(morpheme_files)}ê°œ íŒŒì¼ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
        
        results = []
        all_documents = []
        
        # ê°œë³„ íŒŒì¼ ì²˜ë¦¬
        for i, filename in enumerate(morpheme_files, 1):
            print(f"\n[{i}/{len(morpheme_files)}]")
            result = self.analyze_single_file(filename)
            
            if result:
                results.append(result)
                
                # TF-IDFë¥¼ ìœ„í•œ ë¬¸ì„œ ì¤€ë¹„
                morpheme_path = os.path.join(self.morpheme_folder, filename)
                morpheme_data = self.load_morpheme_file(morpheme_path)
                if morpheme_data:
                    nouns = morpheme_data.get('all_nouns', [])
                    # ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ
                    all_documents.append(' '.join(nouns))
        
        # TF-IDF (ì „ì²´ ë¬¸ì„œ ëŒ€ìƒ)
        if len(all_documents) >= 2:
            print(f"\n\n{'='*60}")
            print(f"ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ (ì „ì²´ ë¬¸ì„œ)")
            print('='*60)
            
            tfidf_results = self.extract_tfidf_keywords(all_documents, top_n=10)
            
            if tfidf_results:
                for i, keywords in enumerate(tfidf_results):
                    print(f"\n  ë¬¸ì„œ {i+1} ({morpheme_files[i]}):")
                    for word, score in keywords[:5]:
                        print(f"    {word}: {score:.4f}")
                
                # TF-IDF ê²°ê³¼ ì €ì¥
                for i, result in enumerate(results):
                    result['tfidf_keywords'] = tfidf_results[i] if i < len(tfidf_results) else []
                    
                    # ì—…ë°ì´íŠ¸ëœ ê²°ê³¼ ì €ì¥
                    output_filename = Path(morpheme_files[i]).stem.replace('_morpheme', '_keywords.json')
                    output_path = os.path.join(self.output_folder, output_filename)
                    
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
        
        # ì „ì²´ í†µê³„
        if results:
            print(f"\n\n{'='*60}")
            print(f"ğŸ“Š ì „ì²´ í‚¤ì›Œë“œ í†µê³„")
            print('='*60)
            
            # ëª¨ë“  ë¬¸ì„œì˜ ë¹ˆë„ í‚¤ì›Œë“œ í•©ì‚°
            all_keywords = []
            for result in results:
                for word, count in result.get('frequency_keywords', []):
                    all_keywords.extend([word] * count)
            
            overall_freq = Counter(all_keywords).most_common(20)
            
            print(f"\n  ğŸ† ì „ì²´ ìƒìœ„ í‚¤ì›Œë“œ (Top 20):")
            for word, count in overall_freq:
                print(f"    {word}: {count}íšŒ")
            
            # ìš”ì•½ ì €ì¥
            summary = {
                'total_files': len(results),
                'overall_top_keywords': overall_freq
            }
            
            summary_path = os.path.join(self.output_folder, 'keywords_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"\n  ğŸ’¾ ì „ì²´ ìš”ì•½ ì €ì¥: {summary_path}")
        
        print(f"\n{'='*60}")
        print(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
        print('='*60)
        
        return results


def main():
    print("\nğŸ”‘ 4ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ")
    
    try:
        extractor = KeywordExtractor()
        
        print("\nì¶”ì¶œ ëª¨ë“œ ì„ íƒ:")
        print("1. ë‹¨ì¼ íŒŒì¼ ì¶”ì¶œ")
        print("2. ì „ì²´ íŒŒì¼ ì¶”ì¶œ")
        
        choice = input("\nì„ íƒ (1-2): ").strip()
        
        if choice == '1':
            filename = input("íŒŒì¼ëª… (ì˜ˆ: EG_001_morpheme.json): ").strip()
            extractor.analyze_single_file(filename)
        elif choice == '2':
            extractor.analyze_all_files()
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒ")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()