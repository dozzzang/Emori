"""
2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ (Mecab ê²½ë¡œ ìë™ ê°ì§€)
"""

import os
import json
from pathlib import Path
from collections import Counter


def init_mecab():
    """Mecab ì´ˆê¸°í™” - ê²½ë¡œ ìë™ ê°ì§€"""
    from konlpy.tag import Mecab
    
    # ì‹œë„í•  ê²½ë¡œ ëª©ë¡
    paths = [
        '/opt/homebrew/lib/mecab/dic/mecab-ko-dic',  # Apple Silicon
        '/usr/local/lib/mecab/dic/mecab-ko-dic',     # Intel Mac
        None  # ê¸°ë³¸ ê²½ë¡œ
    ]
    
    for path in paths:
        try:
            if path:
                mecab = Mecab(path)
            else:
                mecab = Mecab()
            
            # í…ŒìŠ¤íŠ¸
            mecab.morphs("í…ŒìŠ¤íŠ¸")
            
            print(f"âœ… Mecab ì´ˆê¸°í™” ì„±ê³µ (ê²½ë¡œ: {path or 'ê¸°ë³¸'})")
            return mecab
        except Exception as e:
            if path:
                print(f"âš ï¸  ê²½ë¡œ {path} ì‹¤íŒ¨")
            continue
    
    raise Exception("Mecab ì´ˆê¸°í™” ì‹¤íŒ¨!")


class MorphemeAnalyzer:
    """í˜•íƒœì†Œ ë¶„ì„ê¸°"""
    
    def __init__(self, txt_folder="data/txt_files", output_folder="output/morpheme"):
        self.txt_folder = txt_folder
        self.output_folder = output_folder
        os.makedirs(output_folder, exist_ok=True)
        
        print("Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        self.mecab = init_mecab()
        
        self.stopwords = {
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì•½', 'ë˜', 'ì´', 'ê·¸', 'ì €', 'ì œ',
            'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì¢€', 'ë”', 'ë•Œ', 'ê±°', 'ë‚˜', 'ë‚´'
        }
    
    def load_text_file(self, txt_path):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def extract_morphemes(self, text):
        """í˜•íƒœì†Œ ì¶”ì¶œ"""
        pos_tags = self.mecab.pos(text)
        
        nouns = []
        verbs = []
        adjectives = []
        
        for word, pos in pos_tags:
            if word in self.stopwords or len(word) <= 1:
                continue
            
            if pos.startswith('NN'):  # ëª…ì‚¬
                nouns.append(word)
            elif pos.startswith('VV'):  # ë™ì‚¬
                verbs.append(word)
            elif pos.startswith('VA'):  # í˜•ìš©ì‚¬
                adjectives.append(word)
        
        return {
            'all_pos': pos_tags,
            'nouns': nouns,
            'verbs': verbs,
            'adjectives': adjectives
        }
    
    def get_frequency(self, words, top_n=20):
        counter = Counter(words)
        return counter.most_common(top_n)
    
    def analyze_single_file(self, txt_filename):
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        txt_path = os.path.join(self.txt_folder, txt_filename)
        
        if not os.path.exists(txt_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {txt_path}")
            return None
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ë¶„ì„ ì¤‘: {txt_filename}")
        print('='*60)
        
        text = self.load_text_file(txt_path)
        if not text:
            return None
        
        print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        result = self.extract_morphemes(text)
        
        print(f"\n   ğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"      ëª…ì‚¬: {len(result['nouns'])}ê°œ")
        print(f"      ë™ì‚¬: {len(result['verbs'])}ê°œ")
        print(f"      í˜•ìš©ì‚¬: {len(result['adjectives'])}ê°œ")
        
        noun_freq = self.get_frequency(result['nouns'], 10)
        
        print(f"\n   ğŸ” ìƒìœ„ ëª…ì‚¬ (Top 10):")
        for word, count in noun_freq:
            print(f"      {word}: {count}íšŒ")
        
        output_data = {
            'filename': txt_filename,
            'analyzer': 'mecab',
            'text_length': len(text),
            'noun_count': len(result['nouns']),
            'verb_count': len(result['verbs']),
            'adjective_count': len(result['adjectives']),
            'top_nouns': noun_freq,
            'all_nouns': result['nouns'],
            'all_verbs': result['verbs'],
            'all_adjectives': result['adjectives']
        }
        
        output_filename = Path(txt_filename).stem + '_morpheme.json'
        output_path = os.path.join(self.output_folder, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n   ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_data
    
    def analyze_all_files(self):
        """ì „ì²´ íŒŒì¼ ë¶„ì„"""
        txt_files = sorted([f for f in os.listdir(self.txt_folder) if f.endswith('.txt')])
        
        if not txt_files:
            print(f"âŒ TXT íŒŒì¼ ì—†ìŒ: {self.txt_folder}")
            return []
        
        print(f"\nğŸ“š ì´ {len(txt_files)}ê°œ íŒŒì¼ ë¶„ì„ ì‹œì‘")
        
        results = []
        for i, txt_file in enumerate(txt_files, 1):
            print(f"\n[{i}/{len(txt_files)}]")
            result = self.analyze_single_file(txt_file)
            if result:
                results.append(result)
        
        # ì „ì²´ í†µê³„
        if results:
            total_nouns = []
            for result in results:
                total_nouns.extend(result['all_nouns'])
            
            print(f"\n\n{'='*60}")
            print(f"ğŸ“Š ì „ì²´ í†µê³„")
            print('='*60)
            print(f"\n   ì „ì²´ ëª…ì‚¬: {len(total_nouns)}ê°œ (ê³ ìœ : {len(set(total_nouns))}ê°œ)")
            
            print(f"\n   ğŸ† ì „ì²´ ìƒìœ„ ëª…ì‚¬ (Top 20):")
            for word, count in self.get_frequency(total_nouns, 20):
                print(f"      {word}: {count}íšŒ")
            
            summary = {
                'total_files': len(results),
                'analyzer': 'mecab',
                'total_noun_count': len(total_nouns),
                'unique_noun_count': len(set(total_nouns)),
                'top_nouns': self.get_frequency(total_nouns, 50)
            }
            
            summary_path = os.path.join(self.output_folder, 'morpheme_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"\n   ğŸ’¾ ì „ì²´ ìš”ì•½ ì €ì¥: {summary_path}")
        
        print(f"\n{'='*60}")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print('='*60)
        
        return results


def main():
    print("\nğŸ” 2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ (Mecab)")
    
    try:
        analyzer = MorphemeAnalyzer()
        
        print("\n1. ë‹¨ì¼ íŒŒì¼ ë¶„ì„")
        print("2. ì „ì²´ íŒŒì¼ ë¶„ì„")
        
        choice = input("\nì„ íƒ (1-2): ").strip()
        
        if choice == '1':
            filename = input("íŒŒì¼ëª… (ì˜ˆ: EG_001.txt): ").strip()
            analyzer.analyze_single_file(filename)
        elif choice == '2':
            analyzer.analyze_all_files()
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒ")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()