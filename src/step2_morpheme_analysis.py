"""
2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ (Q&A íŒ¨í„´ í•„í„°ë§) - ë¶€ì‚¬/ê°íƒ„ì‚¬ í¬í•¨
Q) A) íŒ¨í„´ì´ ìˆëŠ” ë¬¸ì¥ë§Œ ì¶”ì¶œí•´ì„œ ë¶„ì„
"""

import os
import json
import re
from pathlib import Path
from collections import Counter


def init_mecab():
    # ... (Mecab ì´ˆê¸°í™” ë¡œì§ ìƒëµ) ...
    from konlpy.tag import Mecab
    
    paths = [
        '/opt/homebrew/lib/mecab/dic/mecab-ko-dic',  # Apple Silicon
        '/usr/local/lib/mecab/dic/mecab-ko-dic',     # Intel Mac
        None
    ]
    
    for path in paths:
        try:
            if path:
                mecab = Mecab(path)
            else:
                mecab = Mecab()
            
            mecab.morphs("í…ŒìŠ¤íŠ¸")
            print(f"âœ… Mecab ì´ˆê¸°í™” ì„±ê³µ (ê²½ë¡œ: {path or 'ê¸°ë³¸'})")
            return mecab
        except:
            continue
    
    raise Exception("Mecab ì´ˆê¸°í™” ì‹¤íŒ¨!")


class QAMorphemeAnalyzer:
    """Q&A íŒ¨í„´ í•„í„°ë§ í˜•íƒœì†Œ ë¶„ì„ê¸°"""
    
    def __init__(self, txt_folder="data/txt_files", output_folder="output/morpheme"):
        self.txt_folder = txt_folder
        self.output_folder = output_folder
        os.makedirs(output_folder, exist_ok=True)
        
        print("Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        self.mecab = init_mecab()
        
        self.stopwords = {
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì•½', 'ë˜', 'ì´', 'ê·¸', 'ì €', 'ì œ',
            'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì¢€', 'ë”', 'ë•Œ', 'ê±°', 'ë‚˜', 'ë‚´'
        }
    
    def load_text_file(self, txt_path):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def extract_qa_sections(self, text, include_qa_label=False):
        # ... (Q&A ì„¹ì…˜ ì¶”ì¶œ ë¡œì§ ìƒëµ) ...
        lines = text.split('\n')
        qa_lines = []
        qa_pattern = re.compile(r'^[\s]*(Q|Q\)|Q:|ì§ˆë¬¸|Qï¼š|A|A\)|A:|ë‹µë³€|Aï¼š)', re.IGNORECASE)
        for line in lines:
            if qa_pattern.match(line.strip()):
                qa_lines.append(line)
        
        if include_qa_label:
            return '\n'.join(qa_lines)
        else:
            cleaned_lines = []
            for line in qa_lines:
                cleaned = re.sub(r'^[\s]*(Q|Q\)|Q:|ì§ˆë¬¸|Qï¼š|A|A\)|A:|ë‹µë³€|Aï¼š)\s*', '', line)
                if cleaned.strip():
                    cleaned_lines.append(cleaned)
            return '\n'.join(cleaned_lines)
    
    def get_qa_statistics(self, text):
        # ... (Q&A í†µê³„ ë¡œì§ ìƒëµ) ...
        q_count = len(re.findall(r'(Q\)|Q:|ì§ˆë¬¸)', text, re.IGNORECASE))
        a_count = len(re.findall(r'(A\)|A:|ë‹µë³€)', text, re.IGNORECASE))
        
        return {
            'q_count': q_count,
            'a_count': a_count,
            'total_qa_sections': q_count + a_count
        }

    
    def extract_morphemes(self, text):
        """í˜•íƒœì†Œ ì¶”ì¶œ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬, ê°íƒ„ì‚¬ í¬í•¨)"""
        pos_tags = self.mecab.pos(text)
        
        nouns = []
        verbs = []
        adjectives = []
        adverbs = []
        interjections = []
        
        for word, pos in pos_tags:
            if word in self.stopwords or len(word) <= 1:
                continue
            
            if pos.startswith('NN'):  # ëª…ì‚¬
                nouns.append(word)
            elif pos.startswith('VV'):  # ë™ì‚¬
                verbs.append(word)
            elif pos.startswith('VA'):  # í˜•ìš©ì‚¬
                adjectives.append(word)
            elif pos.startswith('MA'):  # ë¶€ì‚¬ (ì¶”ê°€)
                adverbs.append(word)
            elif pos.startswith('IC'):  # ê°íƒ„ì‚¬ (ì¶”ê°€)
                interjections.append(word)
        
        return {
            'all_pos': pos_tags,
            'nouns': nouns,
            'verbs': verbs,
            'adjectives': adjectives,
            'adverbs': adverbs,
            'interjections': interjections
        }
    
    def get_frequency(self, words, top_n=20):
        counter = Counter(words)
        return counter.most_common(top_n)
    
    def analyze_single_file(self, txt_filename, mode='qa_only'):
        """
        ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        """
        txt_path = os.path.join(self.txt_folder, txt_filename)
        
        if not os.path.exists(txt_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {txt_path}")
            return None
        
        print(f"\n{'='*60}")
        print(f" ë¶„ì„ ì¤‘: {txt_filename}")
        print(f"   ëª¨ë“œ: {'Q&A íŒ¨í„´ë§Œ' if mode == 'qa_only' else 'ì „ì²´ í…ìŠ¤íŠ¸'}")
        print('='*60)
        
        text = self.load_text_file(txt_path)
        if not text:
            return None
        
        # ... (ë¶„ì„ í…ìŠ¤íŠ¸ ì„ íƒ ë¡œì§ ìƒëµ) ...
        if mode == 'qa_only':
            analyze_text = self.extract_qa_sections(text, include_qa_label=False)
            if len(analyze_text) == 0:
                analyze_text = text
                mode = 'all'
        else:
            analyze_text = text

        result = self.extract_morphemes(analyze_text)
        
        # ... (ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë¡œì§ ìƒëµ) ...
        
        output_data = {
            'filename': txt_filename,
            'analyzer': 'mecab_extended',
            'analysis_mode': mode,
            'original_text_length': len(text),
            'analyzed_text_length': len(analyze_text),
            'qa_statistics': self.get_qa_statistics(text),
            'noun_count': len(result['nouns']),
            'verb_count': len(result['verbs']),
            'adjective_count': len(result['adjectives']),
            'adverb_count': len(result['adverbs']),
            'interjection_count': len(result['interjections']),
            'top_nouns': self.get_frequency(result['nouns'], 10),
            'all_nouns': result['nouns'],
            'all_verbs': result['verbs'],
            'all_adjectives': result['adjectives'],
            'all_adverbs': result['adverbs'],
            'all_interjections': result['interjections']
        }
        
        output_filename = Path(txt_filename).stem + '_morpheme.json'
        output_path = os.path.join(self.output_folder, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n    ê²°ê³¼ ì €ì¥: {output_path}")
        return output_data
    
    def analyze_all_files(self, mode='qa_only'):
        # ... (ì „ì²´ íŒŒì¼ ë¶„ì„ ë° ìš”ì•½ ë¡œì§ ìƒëµ) ...
        txt_files = sorted([f for f in os.listdir(self.txt_folder) if f.endswith('.txt')])
        if not txt_files: return []
        
        results = []
        total_nouns = []
        for i, txt_file in enumerate(txt_files, 1):
            result = self.analyze_single_file(txt_file, mode=mode)
            if result:
                results.append(result)
                total_nouns.extend(result['all_nouns'])
        
        if results:
            summary = {
                'total_files': len(results),
                'analyzer': 'mecab_extended',
                'analysis_mode': mode,
                'total_noun_count': len(total_nouns),
                'unique_noun_count': len(set(total_nouns)),
                'top_nouns': self.get_frequency(total_nouns, 50)
            }
            summary_path = os.path.join(self.output_folder, 'morpheme_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
        return results


def main():
    print("\nğŸ” 2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ (Q&A íŒ¨í„´ í•„í„°ë§)")
    try:
        analyzer = QAMorphemeAnalyzer()
        
        print("\në¶„ì„ ëª¨ë“œ ì„ íƒ:")
        mode_choice = input("1. Q&A íŒ¨í„´ë§Œ ë¶„ì„ / 2. ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„ (1-2): ").strip()
        mode = 'qa_only' if mode_choice == '1' else 'all'
        
        choice = input("\nì‹¤í–‰ ëª¨ë“œ ì„ íƒ: 1. ë‹¨ì¼ íŒŒì¼ ë¶„ì„ / 2. ì „ì²´ íŒŒì¼ ë¶„ì„ (1-2): ").strip()
        
        if choice == '1':
            filename = input("íŒŒì¼ëª… (ì˜ˆ: EG_001.txt): ").strip()
            analyzer.analyze_single_file(filename, mode=mode)
        elif choice == '2':
            analyzer.analyze_all_files(mode=mode)
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒ")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()