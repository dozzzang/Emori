"""
2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ (Q&A íŒ¨í„´ í•„í„°ë§)
Q) A) íŒ¨í„´ì´ ìˆëŠ” ë¬¸ì¥ë§Œ ì¶”ì¶œí•´ì„œ ë¶„ì„
"""

import os
import json
import re
from pathlib import Path
from collections import Counter


def init_mecab():
    """Mecab ì´ˆê¸°í™” - ê²½ë¡œ ìë™ ê°ì§€"""
    from konlpy.tag import Mecab
    
    paths = [
        '/opt/homebrew/lib/mecab/dic/mecab-ko-dic',  # Apple Silicon
        '/usr/local/lib/mecab/dic/mecab-ko-dic',     # Intel Mac
        None  # ê¸°ë³¸ ê²½ë¡œ
    ]
    
    for path in paths:
        try:
            if path:
                mecab = Mecab(path)
            else:
                mecab = Mecab()
            
            mecab.morphs("í…ŒìŠ¤íŠ¸")
            print(f"âœ… Mecab ì´ˆê¸°í™” ì„±ê³µ (ê²½ë¡œ: {path or 'ê¸°ë³¸'})")
            return mecab
        except Exception as e:
            if path:
                print(f"âš ï¸  ê²½ë¡œ {path} ì‹¤íŒ¨")
            continue
    
    raise Exception("Mecab ì´ˆê¸°í™” ì‹¤íŒ¨!")


class QAMorphemeAnalyzer:
    """Q&A íŒ¨í„´ í•„í„°ë§ í˜•íƒœì†Œ ë¶„ì„ê¸°"""
    
    def __init__(self, txt_folder="data/txt_files", output_folder="output/morpheme"):
        self.txt_folder = txt_folder
        self.output_folder = output_folder
        os.makedirs(output_folder, exist_ok=True)
        
        print("Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        self.mecab = init_mecab()
        
        self.stopwords = {
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì•½', 'ë˜', 'ì´', 'ê·¸', 'ì €', 'ì œ',
            'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì¢€', 'ë”', 'ë•Œ', 'ê±°', 'ë‚˜', 'ë‚´'
        }
    
    def load_text_file(self, txt_path):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"  âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def extract_qa_sections(self, text, include_qa_label=False):
        """
        Q) A) íŒ¨í„´ ì¶”ì¶œ
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            include_qa_label: Trueë©´ Q), A) ë¼ë²¨ë„ í¬í•¨
        
        Returns:
            Q&A ì„¹ì…˜ë§Œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        # íŒ¨í„´: Q) ë¡œ ì‹œì‘í•˜ê±°ë‚˜ A) ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
        # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›: Q) A) Q: A: Qï¼š Aï¼š ë“±
        
        lines = text.split('\n')
        qa_lines = []
        
        # Q/A íŒ¨í„´ ì •ê·œì‹
        qa_pattern = re.compile(r'^[\s]*(Q|Q\)|Q:|ì§ˆë¬¸|Qï¼š|A|A\)|A:|ë‹µë³€|Aï¼š)', re.IGNORECASE)
        
        for line in lines:
            if qa_pattern.match(line.strip()):
                qa_lines.append(line)
        
        if include_qa_label:
            return '\n'.join(qa_lines)
        else:
            # Q), A) ë¼ë²¨ ì œê±°
            cleaned_lines = []
            for line in qa_lines:
                # Q), A), Q:, A: ë“±ì„ ì œê±°
                cleaned = re.sub(r'^[\s]*(Q|Q\)|Q:|ì§ˆë¬¸|Qï¼š|A|A\)|A:|ë‹µë³€|Aï¼š)\s*', '', line)
                if cleaned.strip():  # ë¹ˆ ì¤„ ì œì™¸
                    cleaned_lines.append(cleaned)
            
            return '\n'.join(cleaned_lines)
    
    def get_qa_statistics(self, text):
        """Q&A í†µê³„ ë¶„ì„"""
        q_count = len(re.findall(r'(Q\)|Q:|ì§ˆë¬¸)', text, re.IGNORECASE))
        a_count = len(re.findall(r'(A\)|A:|ë‹µë³€)', text, re.IGNORECASE))
        
        return {
            'q_count': q_count,
            'a_count': a_count,
            'total_qa_sections': q_count + a_count
        }
    
    def extract_morphemes(self, text):
        """í˜•íƒœì†Œ ì¶”ì¶œ"""
        pos_tags = self.mecab.pos(text)
        
        nouns = []
        verbs = []
        adjectives = []
        
        for word, pos in pos_tags:
            if word in self.stopwords or len(word) <= 1:
                continue
            
            if pos.startswith('NN'):  # ëª…ì‚¬
                nouns.append(word)
            elif pos.startswith('VV'):  # ë™ì‚¬
                verbs.append(word)
            elif pos.startswith('VA'):  # í˜•ìš©ì‚¬
                adjectives.append(word)
        
        return {
            'all_pos': pos_tags,
            'nouns': nouns,
            'verbs': verbs,
            'adjectives': adjectives
        }
    
    def get_frequency(self, words, top_n=20):
        counter = Counter(words)
        return counter.most_common(top_n)
    
    def analyze_single_file(self, txt_filename, mode='qa_only'):
        """
        ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        
        Args:
            txt_filename: íŒŒì¼ëª…
            mode: 'qa_only' (Q&Aë§Œ), 'all' (ì „ì²´)
        """
        txt_path = os.path.join(self.txt_folder, txt_filename)
        
        if not os.path.exists(txt_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {txt_path}")
            return None
        
        print(f"\n{'='*60}")
        print(f" ë¶„ì„ ì¤‘: {txt_filename}")
        print(f"   ëª¨ë“œ: {'Q&A íŒ¨í„´ë§Œ' if mode == 'qa_only' else 'ì „ì²´ í…ìŠ¤íŠ¸'}")
        print('='*60)
        
        text = self.load_text_file(txt_path)
        if not text:
            return None
        
        print(f"   ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # Q&A í†µê³„
        qa_stats = self.get_qa_statistics(text)
        print(f"   Q&A ì„¹ì…˜: Q) {qa_stats['q_count']}íšŒ, A) {qa_stats['a_count']}íšŒ")
        
        # ë¶„ì„í•  í…ìŠ¤íŠ¸ ì„ íƒ
        if mode == 'qa_only':
            analyze_text = self.extract_qa_sections(text, include_qa_label=False)
            print(f"   ì¶”ì¶œëœ Q&A í…ìŠ¤íŠ¸: {len(analyze_text)} ë¬¸ì")
            
            if len(analyze_text) == 0:
                print("   âš ï¸  Q&A íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   ì „ì²´ í…ìŠ¤íŠ¸ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
                analyze_text = text
                mode = 'all'
        else:
            analyze_text = text
        
        result = self.extract_morphemes(analyze_text)
        
        print(f"\n    ë¶„ì„ ê²°ê³¼:")
        print(f"      ëª…ì‚¬: {len(result['nouns'])}ê°œ")
        print(f"      ë™ì‚¬: {len(result['verbs'])}ê°œ")
        print(f"      í˜•ìš©ì‚¬: {len(result['adjectives'])}ê°œ")
        
        noun_freq = self.get_frequency(result['nouns'], 10)
        
        print(f"\n   ğŸ” ìƒìœ„ ëª…ì‚¬ (Top 10):")
        for word, count in noun_freq:
            print(f"      {word}: {count}íšŒ")
        
        output_data = {
            'filename': txt_filename,
            'analyzer': 'mecab_qa_filtered',
            'analysis_mode': mode,
            'original_text_length': len(text),
            'analyzed_text_length': len(analyze_text),
            'qa_statistics': qa_stats,
            'noun_count': len(result['nouns']),
            'verb_count': len(result['verbs']),
            'adjective_count': len(result['adjectives']),
            'top_nouns': noun_freq,
            'all_nouns': result['nouns'],
            'all_verbs': result['verbs'],
            'all_adjectives': result['adjectives']
        }
        
        output_filename = Path(txt_filename).stem + '_morpheme.json'
        output_path = os.path.join(self.output_folder, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n    ê²°ê³¼ ì €ì¥: {output_path}")
        return output_data
    
    def analyze_all_files(self, mode='qa_only'):
        """ì „ì²´ íŒŒì¼ ë¶„ì„"""
        txt_files = sorted([f for f in os.listdir(self.txt_folder) if f.endswith('.txt')])
        
        if not txt_files:
            print(f"âŒ TXT íŒŒì¼ ì—†ìŒ: {self.txt_folder}")
            return []
        
        print(f"\n ì´ {len(txt_files)}ê°œ íŒŒì¼ ë¶„ì„ ì‹œì‘")
        print(f"   ëª¨ë“œ: {'Q&A íŒ¨í„´ë§Œ' if mode == 'qa_only' else 'ì „ì²´ í…ìŠ¤íŠ¸'}")
        
        results = []
        for i, txt_file in enumerate(txt_files, 1):
            print(f"\n[{i}/{len(txt_files)}]")
            result = self.analyze_single_file(txt_file, mode=mode)
            if result:
                results.append(result)
        
        # ì „ì²´ í†µê³„
        if results:
            total_nouns = []
            for result in results:
                total_nouns.extend(result['all_nouns'])
            
            print(f"\n\n{'='*60}")
            print(f" ì „ì²´ í†µê³„")
            print('='*60)
            print(f"\n   ì „ì²´ ëª…ì‚¬: {len(total_nouns)}ê°œ (ê³ ìœ : {len(set(total_nouns))}ê°œ)")
            
            print(f"\n    ì „ì²´ ìƒìœ„ ëª…ì‚¬ (Top 20):")
            for word, count in self.get_frequency(total_nouns, 20):
                print(f"      {word}: {count}íšŒ")
            
            summary = {
                'total_files': len(results),
                'analyzer': 'mecab_qa_filtered',
                'analysis_mode': mode,
                'total_noun_count': len(total_nouns),
                'unique_noun_count': len(set(total_nouns)),
                'top_nouns': self.get_frequency(total_nouns, 50)
            }
            
            summary_path = os.path.join(self.output_folder, 'morpheme_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"\n    ì „ì²´ ìš”ì•½ ì €ì¥: {summary_path}")
        
        print(f"\n{'='*60}")
        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print('='*60)
        
        return results


def main():
    print("\nğŸ” 2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„ (Q&A íŒ¨í„´ í•„í„°ë§)")
    
    try:
        analyzer = QAMorphemeAnalyzer()
        
        print("\në¶„ì„ ëª¨ë“œ ì„ íƒ:")
        print("1. Q&A íŒ¨í„´ë§Œ ë¶„ì„ (Q), A) ë¶€ë¶„ë§Œ)")
        print("2. ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„")
        
        mode_choice = input("\nì„ íƒ (1-2): ").strip()
        mode = 'qa_only' if mode_choice == '1' else 'all'
        
        print("\nì‹¤í–‰ ëª¨ë“œ ì„ íƒ:")
        print("1. ë‹¨ì¼ íŒŒì¼ ë¶„ì„")
        print("2. ì „ì²´ íŒŒì¼ ë¶„ì„")
        
        choice = input("\nì„ íƒ (1-2): ").strip()
        
        if choice == '1':
            filename = input("íŒŒì¼ëª… (ì˜ˆ: EG_001.txt): ").strip()
            analyzer.analyze_single_file(filename, mode=mode)
        elif choice == '2':
            analyzer.analyze_all_files(mode=mode)
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒ")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()