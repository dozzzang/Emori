"""
3ë‹¨ê³„: ê°ì • ë¶„ì„ (í†µí•© ë²„ì „)
- ë°©ë²• 1: KNU ê°ì •ì‚¬ì „ ê¸°ë°˜ (ë¹ ë¦„, ê·œì¹™ ê¸°ë°˜)
- ë°©ë²• 2: BERT ë”¥ëŸ¬ë‹ ëª¨ë¸ (ëŠë¦¼, ì •í™•)
"""

import os
import json
from pathlib import Path
from collections import Counter
import urllib.request
import warnings
warnings.filterwarnings('ignore')


class SentimentAnalyzer:
    """ê°ì • ë¶„ì„ê¸° - ì‚¬ì „ ê¸°ë°˜ + BERT"""
    
    def __init__(self, morpheme_folder="output/morpheme", output_folder="output/sentiment", use_bert=False):
        self.morpheme_folder = morpheme_folder
        self.output_folder = output_folder
        self.use_bert = use_bert
        os.makedirs(output_folder, exist_ok=True)
        
        print("ê°ì • ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # 1. ê°ì •ì‚¬ì „ ë¡œë“œ
        self.sentiment_dict = self._load_sentiment_lexicon()
        
        # 2. BERT ëª¨ë¸ ë¡œë“œ (ì˜µì…˜)
        self.bert_analyzer = None
        if use_bert:
            self.bert_analyzer = self._load_bert_model()
        
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    def _download_lexicon(self):
        """KNU ê°ì •ì‚¬ì „ ë‹¤ìš´ë¡œë“œ"""
        url = "https://raw.githubusercontent.com/park1200656/KnuSentiLex/master/SentiWord_Dict.txt"
        
        lexicon_dir = "data/sentiment"
        os.makedirs(lexicon_dir, exist_ok=True)
        
        lexicon_path = os.path.join(lexicon_dir, "SentiWord_Dict.txt")
        
        if os.path.exists(lexicon_path):
            return lexicon_path
        
        print(" KNU ê°ì •ì‚¬ì „ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        try:
            urllib.request.urlretrieve(url, lexicon_path)
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            return lexicon_path
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_sentiment_lexicon(self):
        """ê°ì •ì‚¬ì „ ë¡œë“œ"""
        lexicon_path = self._download_lexicon()
        
        if not lexicon_path:
            print("âš ï¸  ê¸°ë³¸ ê°ì •ë‹¨ì–´ ì‚¬ìš©")
            return self._get_basic_sentiment_dict()
        
        sentiment_dict = {}
        
        try:
            with open(lexicon_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        parts = line.strip().split('\t')
                        if len(parts) >= 2:
                            word = parts[0]
                            try:
                                polarity = float(parts[1])
                                sentiment_dict[word] = polarity
                            except:
                                continue
            
            print(f"âœ… ê°ì •ì‚¬ì „ ë¡œë“œ: {len(sentiment_dict)}ê°œ ë‹¨ì–´")
            return sentiment_dict
        
        except Exception as e:
            print(f"âš ï¸  ê°ì •ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._get_basic_sentiment_dict()
    
    def _get_basic_sentiment_dict(self):
        """ê¸°ë³¸ ê°ì • ë‹¨ì–´ ì‚¬ì „"""
        return {
            # ê¸ì •
            'ì¢‹ë‹¤': 1.0, 'í–‰ë³µí•˜ë‹¤': 1.0, 'í¸ì•ˆí•˜ë‹¤': 1.0, 'ì¦ê²ë‹¤': 1.0,
            'ê¸°ì˜ë‹¤': 1.0, 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤': 1.0, 'í¸í•˜ë‹¤': 1.0, 'ì¬ë¯¸ìˆë‹¤': 1.0,
            'í›Œë¥­í•˜ë‹¤': 1.0, 'ë©‹ì§€ë‹¤': 1.0, 'ê°ì‚¬í•˜ë‹¤': 1.0,
            
            # ë¶€ì •
            'ë‚˜ì˜ë‹¤': -1.0, 'ë¶ˆì•ˆí•˜ë‹¤': -1.0, 'ìŠ¬í”„ë‹¤': -1.0, 'í˜ë“¤ë‹¤': -1.0,
            'ìš°ìš¸í•˜ë‹¤': -1.0, 'ìŠ¤íŠ¸ë ˆìŠ¤': -1.0, 'ë¶ˆí¸í•˜ë‹¤': -1.0, 'ë‹µë‹µí•˜ë‹¤': -1.0,
            'ë¬´ì„­ë‹¤': -1.0, 'ê±±ì •ë˜ë‹¤': -1.0, 'ì§œì¦ë‚˜ë‹¤': -1.0
        }
    
    def _load_bert_model(self):
        """BERT ëª¨ë¸ ë¡œë“œ"""
        print("\nğŸ¤– BERT ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ)")
        
        try:
            from transformers import pipeline
            
            # í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ë“¤
            models = [
                "matthewburke/korean_sentiment",  # ì¶”ì²œ
                "snunlp/KR-ELECTRA-discriminator",
                "beomi/kcbert-base"
            ]
            
            for model_name in models:
                try:
                    analyzer = pipeline(
                        "sentiment-analysis",
                        model=model_name,
                        tokenizer=model_name
                    )
                    print(f"âœ… BERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_name}")
                    return analyzer
                except Exception as e:
                    print(f"âš ï¸  {model_name} ì‹¤íŒ¨: {e}")
                    continue
            
            print("âŒ ëª¨ë“  BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì‚¬ì „ ê¸°ë°˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return None
            
        except ImportError:
            print("âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜: pip install transformers torch")
            return None
    
    def analyze_lexicon_based(self, words):
        """ì‚¬ì „ ê¸°ë°˜ ê°ì • ë¶„ì„"""
        
        scores = []
        positive_words = []
        negative_words = []
        
        for word in words:
            if word in self.sentiment_dict:
                score = self.sentiment_dict[word]
                scores.append(score)
                
                if score > 0:
                    positive_words.append((word, score))
                elif score < 0:
                    negative_words.append((word, score))
        
        if scores:
            avg_score = sum(scores) / len(scores)
            total_score = sum(scores)
        else:
            avg_score = 0
            total_score = 0
        
        # ê°ì • ë¶„ë¥˜
        if avg_score > 0.1:
            sentiment = "ê¸ì •"
        elif avg_score < -0.1:
            sentiment = "ë¶€ì •"
        else:
            sentiment = "ì¤‘ë¦½"
        
        return {
            'method': 'lexicon',
            'sentiment': sentiment,
            'avg_score': round(avg_score, 3),
            'total_score': round(total_score, 3),
            'positive_words': sorted(positive_words, key=lambda x: x[1], reverse=True),
            'negative_words': sorted(negative_words, key=lambda x: x[1]),
            'emotion_word_count': len(scores)
        }
    
    def analyze_bert_based(self, text):
        """BERT ê¸°ë°˜ ê°ì • ë¶„ì„"""
        
        if not self.bert_analyzer:
            return None
        
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ëƒ„ (BERTëŠ” 512 í† í° ì œí•œ)
            if len(text) > 500:
                text = text[:500]
            
            result = self.bert_analyzer(text)[0]
            
            # labelì„ í•œê¸€ë¡œ ë³€í™˜
            label_map = {
                'POSITIVE': 'ê¸ì •',
                'NEGATIVE': 'ë¶€ì •',
                'NEUTRAL': 'ì¤‘ë¦½',
                'positive': 'ê¸ì •',
                'negative': 'ë¶€ì •',
                'neutral': 'ì¤‘ë¦½'
            }
            
            sentiment = label_map.get(result['label'], result['label'])
            confidence = result['score']
            
            return {
                'method': 'bert',
                'sentiment': sentiment,
                'confidence': round(confidence, 3)
            }
        
        except Exception as e:
            print(f"   âš ï¸  BERT ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_single_file(self, morpheme_filename):
        """ë‹¨ì¼ íŒŒì¼ ê°ì • ë¶„ì„"""
        
        morpheme_path = os.path.join(self.morpheme_folder, morpheme_filename)
        
        if not os.path.exists(morpheme_path):
            print(f"âŒ íŒŒì¼ ì—†ìŒ: {morpheme_path}")
            return None
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ê°ì • ë¶„ì„ ì¤‘: {morpheme_filename}")
        print('='*60)
        
        # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ë¡œë“œ
        try:
            with open(morpheme_path, 'r', encoding='utf-8') as f:
                morpheme_data = json.load(f)
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ê²½ë¡œ ì¶”ì •
        txt_filename = morpheme_data.get('filename', '')
        txt_path = os.path.join('data/txt_files', txt_filename)
        
        original_text = ""
        if os.path.exists(txt_path):
            with open(txt_path, 'r', encoding='utf-8') as f:
                original_text = f.read()
        
        # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘
        all_words = []
        all_words.extend(morpheme_data.get('all_nouns', []))
        all_words.extend(morpheme_data.get('all_verbs', []))
        all_words.extend(morpheme_data.get('all_adjectives', []))
        
        print(f"   ë¶„ì„í•  ë‹¨ì–´ ìˆ˜: {len(all_words)}ê°œ")
        
        # 1. ì‚¬ì „ ê¸°ë°˜ ë¶„ì„
        print(f"\n    ì‚¬ì „ ê¸°ë°˜ ë¶„ì„ ì¤‘...")
        lexicon_result = self.analyze_lexicon_based(all_words)
        
        print(f"      ê°ì •: {lexicon_result['sentiment']}")
        print(f"      í‰ê·  ì ìˆ˜: {lexicon_result['avg_score']}")
        print(f"      ê°ì • ë‹¨ì–´: {lexicon_result['emotion_word_count']}ê°œ")
        
        if lexicon_result['positive_words']:
            print(f"\n       ê¸ì • ë‹¨ì–´ (Top 5):")
            for word, score in lexicon_result['positive_words'][:5]:
                print(f"         {word}: +{score}")
        
        if lexicon_result['negative_words']:
            print(f"\n       ë¶€ì • ë‹¨ì–´ (Top 5):")
            for word, score in lexicon_result['negative_words'][:5]:
                print(f"         {word}: {score}")
        
        # 2. BERT ê¸°ë°˜ ë¶„ì„ (ì˜µì…˜)
        bert_result = None
        if self.use_bert and original_text:
            print(f"\n    BERT ë¶„ì„ ì¤‘...")
            bert_result = self.analyze_bert_based(original_text)
            
            if bert_result:
                print(f"      ê°ì •: {bert_result['sentiment']}")
                print(f"      ì‹ ë¢°ë„: {bert_result['confidence']}")
        
        # ê²°ê³¼ ì €ì¥
        output_data = {
            'filename': txt_filename,
            'lexicon_based': lexicon_result,
            'bert_based': bert_result,
            'text_length': morpheme_data.get('text_length', 0)
        }
        
        output_filename = Path(morpheme_filename).stem.replace('_morpheme', '_sentiment.json')
        output_path = os.path.join(self.output_folder, output_filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n    ê²°ê³¼ ì €ì¥: {output_path}")
        
        return output_data
    
    def analyze_all_files(self):
        """ëª¨ë“  íŒŒì¼ ê°ì • ë¶„ì„"""
        
        morpheme_files = sorted([
            f for f in os.listdir(self.morpheme_folder) 
            if f.endswith('_morpheme.json')
        ])
        
        if not morpheme_files:
            print(f"âŒ í˜•íƒœì†Œ ë¶„ì„ íŒŒì¼ ì—†ìŒ: {self.morpheme_folder}")
            print("   ë¨¼ì € Step 2ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!")
            return []
        
        print(f"\nğŸ“š ì´ {len(morpheme_files)}ê°œ íŒŒì¼ ê°ì • ë¶„ì„ ì‹œì‘")
        print(f"   ë°©ë²•: {'ì‚¬ì „ + BERT' if self.use_bert else 'ì‚¬ì „ ê¸°ë°˜'}")
        
        results = []
        for i, filename in enumerate(morpheme_files, 1):
            print(f"\n[{i}/{len(morpheme_files)}]")
            result = self.analyze_single_file(filename)
            if result:
                results.append(result)
        
        # ì „ì²´ í†µê³„
        if results:
            print(f"\n\n{'='*60}")
            print(f" ì „ì²´ í†µê³„")
            print('='*60)
            
            # ì‚¬ì „ ê¸°ë°˜ í†µê³„
            lexicon_sentiments = [r['lexicon_based']['sentiment'] for r in results]
            lexicon_counts = Counter(lexicon_sentiments)
            
            print(f"\n   [ì‚¬ì „ ê¸°ë°˜] ê°ì • ë¶„í¬:")
            for sentiment, count in lexicon_counts.items():
                percentage = (count / len(results)) * 100
                print(f"      {sentiment}: {count}ê°œ ({percentage:.1f}%)")
            
            avg_scores = [r['lexicon_based']['avg_score'] for r in results]
            overall_avg = sum(avg_scores) / len(avg_scores)
            print(f"\n   ì „ì²´ í‰ê·  ê°ì • ì ìˆ˜: {overall_avg:.3f}")
            
            # BERT í†µê³„
            if self.use_bert:
                bert_sentiments = [
                    r['bert_based']['sentiment'] 
                    for r in results 
                    if r['bert_based']
                ]
                if bert_sentiments:
                    bert_counts = Counter(bert_sentiments)
                    print(f"\n   [BERT ê¸°ë°˜] ê°ì • ë¶„í¬:")
                    for sentiment, count in bert_counts.items():
                        percentage = (count / len(bert_sentiments)) * 100
                        print(f"      {sentiment}: {count}ê°œ ({percentage:.1f}%)")
            
            # ìš”ì•½ ì €ì¥
            summary = {
                'total_files': len(results),
                'method': 'lexicon + bert' if self.use_bert else 'lexicon',
                'lexicon_distribution': dict(lexicon_counts),
                'overall_avg_score': round(overall_avg, 3),
                'files': [
                    {
                        'filename': r['filename'],
                        'lexicon_sentiment': r['lexicon_based']['sentiment'],
                        'lexicon_score': r['lexicon_based']['avg_score'],
                        'bert_sentiment': r['bert_based']['sentiment'] if r['bert_based'] else None,
                        'bert_confidence': r['bert_based']['confidence'] if r['bert_based'] else None
                    }
                    for r in results
                ]
            }
            
            summary_path = os.path.join(self.output_folder, 'sentiment_summary.json')
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"\n    ì „ì²´ ìš”ì•½ ì €ì¥: {summary_path}")
        
        print(f"\n{'='*60}")
        print(f"âœ… ê°ì • ë¶„ì„ ì™„ë£Œ!")
        print('='*60)
        
        return results


def main():
    print("\n 3ë‹¨ê³„: ê°ì • ë¶„ì„ (í†µí•©)")
    
    print("\në¶„ì„ ë°©ë²• ì„ íƒ:")
    print("1. ì‚¬ì „ ê¸°ë°˜ë§Œ (ë¹ ë¦„)")
    print("2. ì‚¬ì „ + BERT (ëŠë¦¼, ì •í™•)")
    
    method_choice = input("\nì„ íƒ (1-2): ").strip()
    use_bert = (method_choice == '2')
    
    try:
        analyzer = SentimentAnalyzer(use_bert=use_bert)
        
        print("\në¶„ì„ ëª¨ë“œ ì„ íƒ:")
        print("1. ë‹¨ì¼ íŒŒì¼ ë¶„ì„")
        print("2. ì „ì²´ íŒŒì¼ ë¶„ì„")
        
        choice = input("\nì„ íƒ (1-2): ").strip()
        
        if choice == '1':
            filename = input("íŒŒì¼ëª… (ì˜ˆ: EG_001_morpheme.json): ").strip()
            analyzer.analyze_single_file(filename)
        elif choice == '2':
            analyzer.analyze_all_files()
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒ")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()